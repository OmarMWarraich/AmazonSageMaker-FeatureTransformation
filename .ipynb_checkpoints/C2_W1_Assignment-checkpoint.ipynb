{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature transformation with Amazon SageMaker processing job and Feature Store\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this lab you will start with the raw [Women's Clothing Reviews](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews) dataset and prepare it to train a BERT-based natural language processing (NLP) model. The model will be used to classify customer reviews into positive (1), neutral (0) and negative (-1) sentiment.\n",
    "\n",
    "You will convert the original review text into machine-readable features used by BERT. To perform the required feature transformation you will configure an Amazon SageMaker processing job, which will be running a custom Python script.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [1. Configure the SageMaker Feature Store](#c2w1-1.)\n",
    "  - [1.1. Configure dataset](#c2w1-1.1.)\n",
    "  - [1.2. Configure the SageMaker feature store](#c2w1-1.2.)\n",
    "    - [Exercise 1](#c2w1-ex-1)\n",
    "- [2. Transform the dataset](#c2w1-2.)\n",
    "    - [Exercise 2](#c2w1-ex-2)\n",
    "    - [Exercise 3](#c2w1-ex-3)\n",
    "- [3. Query the Feature Store](#c2w1-3.)\n",
    "  - [3.1. Export training, validation, and test datasets from the Feature Store](#c2w1-3.1.)\n",
    "    - [Exercise 4](#c2w1-ex-4)\n",
    "  - [3.2. Export TSV from Feature Store](#c2w1-3.2.)\n",
    "  - [3.3. Check that the dataset in the Feature Store is balanced by sentiment](#c2w1-3.3.)\n",
    "    - [Exercise 5](#c2w1-ex-5)\n",
    "    - [Exercise 6](#c2w1-ex-6)\n",
    "    - [Exercise 7](#c2w1-ex-7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch==1.6.0\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2021.5.25  |       h06a4308_1         112 KB\n",
      "    certifi-2021.5.30          |   py37h06a4308_0         139 KB\n",
      "    conda-4.10.3               |   py37h06a4308_0         2.9 MB\n",
      "    cudatoolkit-10.2.89        |       hfd86e86_1       365.1 MB\n",
      "    ninja-1.10.2               |       hff7bd54_1         1.4 MB\n",
      "    pytorch-1.6.0              |py3.7_cuda10.2.89_cudnn7.6.5_0       537.7 MB  pytorch\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       907.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cudatoolkit        pkgs/main/linux-64::cudatoolkit-10.2.89-hfd86e86_1\n",
      "  ninja              pkgs/main/linux-64::ninja-1.10.2-hff7bd54_1\n",
      "  pytorch            pytorch/linux-64::pytorch-1.6.0-py3.7_cuda10.2.89_cudnn7.6.5_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2020.12.~ --> pkgs/main::ca-certificates-2021.5.25-h06a4308_1\n",
      "  certifi            conda-forge::certifi-2020.12.5-py37h8~ --> pkgs/main::certifi-2021.5.30-py37h06a4308_0\n",
      "  conda              conda-forge::conda-4.10.1-py37h89c186~ --> pkgs/main::conda-4.10.3-py37h06a4308_0\n",
      "\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# please ignore warning messages during the installation\n",
    "!pip install --disable-pip-version-check -q sagemaker==2.35.0\n",
    "!conda install -q -y pytorch==1.6.0 -c pytorch\n",
    "!pip install --disable-pip-version-check -q transformers==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-1.'></a>\n",
    "# 1. Configure the SageMaker Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-1.1.'></a>\n",
    "### 1.1. Configure dataset\n",
    "The raw dataset is in the public S3 bucket. Let's start by specifying the S3 location of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://dlai-practical-data-science/data/raw/\n"
     ]
    }
   ],
   "source": [
    "raw_input_data_s3_uri = 's3://dlai-practical-data-science/data/raw/'\n",
    "print(raw_input_data_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the files in the S3 bucket (in this case it will be just one file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-30 02:21:06    8457214 womens_clothing_ecommerce_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $raw_input_data_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-1.2.'></a>\n",
    "### 1.2. Configure the SageMaker feature store\n",
    "\n",
    "As the result of the transformation, in addition to generating files in S3 bucket, you will also save the transformed data in the **Amazon SageMaker Feature Store** to be used by others in your organization, for example. \n",
    "\n",
    "To configure a Feature Store you need to setup a **Feature Group**. This is the main resource containing all of the metadata related to the data stored in the Feature Store. A Feature Group should contain a list of **Feature Definitions**. A Feature Definition consists of a name and the data type. The Feature Group also contains an online store configuration and an offline store configuration controlling where the data is stored. Enabling the online store allows quick access to the latest value for a record via the [GetRecord API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_feature_store_GetRecord.html). The offline store allows storage of the data in your S3 bucket. You will be using the offline store in this lab.\n",
    "\n",
    "Let's setup the Feature Group name and the Feature Store offline prefix in S3 bucket (you will use those later in the lab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature group name: reviews-feature-group-1625545514\n",
      "Feature store offline prefix in S3: reviews-feature-store-1625545514\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "timestamp = int(time.time())\n",
    "\n",
    "feature_group_name = 'reviews-feature-group-' + str(timestamp)\n",
    "feature_store_offline_prefix = 'reviews-feature-store-' + str(timestamp)\n",
    "\n",
    "print('Feature group name: {}'.format(feature_group_name))\n",
    "print('Feature store offline prefix in S3: {}'.format(feature_store_offline_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking two features from the original raw dataset (`Review Text` and `Rating`), you will transform it preparing to be used for the model training and then to be saved in the Feature Store. Here you will define the related features to be stored as a list of `FeatureDefinition`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_definition import (\n",
    "    FeatureDefinition,\n",
    "    FeatureTypeEnum,\n",
    ")\n",
    "\n",
    "feature_definitions= [\n",
    "    # unique ID of the review\n",
    "    FeatureDefinition(feature_name='review_id', feature_type=FeatureTypeEnum.STRING), \n",
    "    # ingestion timestamp\n",
    "    FeatureDefinition(feature_name='date', feature_type=FeatureTypeEnum.STRING),\n",
    "    # sentiment: -1 (negative), 0 (neutral) or 1 (positive). It will be found the Rating values (1, 2, 3, 4, 5)\n",
    "    FeatureDefinition(feature_name='sentiment', feature_type=FeatureTypeEnum.STRING), \n",
    "    # label ID of the target class (sentiment)\n",
    "    FeatureDefinition(feature_name='label_id', feature_type=FeatureTypeEnum.STRING),\n",
    "    # reviews encoded with the BERT tokenizer\n",
    "    FeatureDefinition(feature_name='input_ids', feature_type=FeatureTypeEnum.STRING),\n",
    "    # original Review Text\n",
    "    FeatureDefinition(feature_name='review_body', feature_type=FeatureTypeEnum.STRING),\n",
    "    # train/validation/test label\n",
    "    FeatureDefinition(feature_name='split_type', feature_type=FeatureTypeEnum.STRING)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-ex-1'></a>\n",
    "### Exercise 1\n",
    "\n",
    "Create the feature group using the feature definitions defined above.\n",
    "\n",
    "**Instructions:** Use the `FeatureGroup` function passing the defined above feature group name and the feature definitions.\n",
    "\n",
    "```python\n",
    "feature_group = FeatureGroup(\n",
    "    name=..., # Feature Group name\n",
    "    feature_definitions=..., # a list of Feature Definitions\n",
    "    sagemaker_session=sess # SageMaker session\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroup(name='reviews-feature-group-1625545514', sagemaker_session=<sagemaker.session.Session object at 0x7f6ca8744cd0>, feature_definitions=[FeatureDefinition(feature_name='review_id', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='date', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='sentiment', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='label_id', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='input_ids', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='review_body', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='split_type', feature_type=<FeatureTypeEnum.STRING: 'String'>)])\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "feature_group = FeatureGroup(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    name= 'reviews-feature-group-1625545514', # Replace None\n",
    "    feature_definitions= feature_definitions, # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "print(feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the defined Feature Group later in this lab, the actual creation of the Feature Group will take place in the processing job. Now let's move into the setup of the processing job to transform the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-2.'></a>\n",
    "# 2. Transform the dataset\n",
    "\n",
    "You will configure a SageMaker processing job to run a custom Python script to balance and transform the raw data into a format used by BERT model.\n",
    "\n",
    "Set the transformation parameters including the instance type, instance count, and train/validation/test split percentages. You can also choose whether you want to balance the dataset or not. In this case, you will balance the dataset to avoid class imbalance in the target variable, `sentiment`. \n",
    "\n",
    "Another important parameter of the model is the `max_seq_length`, which specifies the maximum length of the classified reviews for the RoBERTa model. If the sentence is shorter than the maximum length parameter, it will be padded. In another case, when the sentence is longer, it will be truncated from the right side.\n",
    "\n",
    "Since a smaller `max_seq_length` leads to faster training and lower resource utilization, you want to find the smallest power-of-2 that captures `100%` of our reviews.  For this dataset, the `100th` percentile is `115`.  However, it's best to stick with powers-of-2 when using BERT. So let's choose `128` as this is the smallest power-of-2 greater than `115`. You will see below how the shorter sentences will be padded to a maximum length.\n",
    "\n",
    "\n",
    "```\n",
    "mean        52.512374\n",
    "std         31.387048\n",
    "min          1.000000\n",
    "10%         10.000000\n",
    "20%         22.000000\n",
    "30%         32.000000\n",
    "40%         41.000000\n",
    "50%         51.000000\n",
    "60%         61.000000\n",
    "70%         73.000000\n",
    "80%         88.000000\n",
    "90%         97.000000\n",
    "100%       115.000000\n",
    "max        115.000000\n",
    "```\n",
    "\n",
    "![](images/distribution_num_words_per_review.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processing_instance_type='ml.c5.xlarge'\n",
    "processing_instance_count=1\n",
    "train_split_percentage=0.90\n",
    "validation_split_percentage=0.05\n",
    "test_split_percentage=0.05\n",
    "balance_dataset=True\n",
    "max_seq_length=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To balance and transform our data, we will use a scikit-learn-based processing job. This is essentially a generic Python processing job with scikit-learn pre-installed.  We can specify the version of scikit-learn we wish to use. Also pass the SageMaker execution role, processing instance type and instance count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    env={'AWS_DEFAULT_REGION': region},                             \n",
    "    max_runtime_in_seconds=7200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processing job will be running the Python code from the file `src/prepare_data.py`. In the following exercise you will review the contents of the file and familiarize yourself with main parts of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-ex-2'></a>\n",
    "### Exercise 2\n",
    "\n",
    "1. Open the file [src/prepare_data.py](src/prepare_data.py). Go through the comments to understand its content.\n",
    "2. Find and review the `convert_to_bert_input_ids()` function, which contains the RoBERTa `tokenizer` configuration.\n",
    "3. Complete method `encode_plus` of the RoBERTa `tokenizer`. Pass the `max_seq_length` as a value for the argument `max_length`. It defines a pad to a maximum length specified.\n",
    "4. Save the file [src/prepare_data.py](src/prepare_data.py) (with the menu command File -> Save Python File)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _This cell will take approximately 1-2 minutes to run._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "Updated correctly!\n",
      "##################\n"
     ]
    }
   ],
   "source": [
    "import sys, importlib\n",
    "sys.path.append('src/')\n",
    "\n",
    "# import the `prepare_data.py` module\n",
    "import prepare_data\n",
    "\n",
    "# reload the module if it has been previously loaded \n",
    "if 'prepare_data' in sys.modules:\n",
    "    importlib.reload(prepare_data)\n",
    "\n",
    "input_ids = prepare_data.convert_to_bert_input_ids(\"this product is great!\", max_seq_length)\n",
    "    \n",
    "updated_correctly = False\n",
    "\n",
    "if len(input_ids) != max_seq_length:\n",
    "    print('#######################################################################################################')\n",
    "    print('Please check that the function \\'convert_to_bert_input_ids\\' in the file src/prepare_data.py is complete.')\n",
    "    print('#######################################################################################################')\n",
    "    raise Exception('Please check that the function \\'convert_to_bert_input_ids\\' in the file src/prepare_data.py is complete.')\n",
    "else:\n",
    "    print('##################')\n",
    "    print('Updated correctly!')\n",
    "    print('##################')\n",
    "\n",
    "    updated_correctly = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the results of tokenization for the given example (*\\\"this product is great!\\\"*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 9226, 1152, 16, 372, 328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Length of the sequence: 128\n"
     ]
    }
   ],
   "source": [
    "input_ids = prepare_data.convert_to_bert_input_ids(\"this product is great!\", max_seq_length)\n",
    "\n",
    "print(input_ids)\n",
    "print('Length of the sequence: {}'.format(len(input_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the processing job with the custom script passing defined above parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2021-07-06-04-31-14-301\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://dlai-practical-data-science/data/raw/', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-387108533973/sagemaker-scikit-learn-2021-07-06-04-31-14-301/input/code/prepare_data.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'sentiment-train', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-387108533973/sagemaker-scikit-learn-2021-07-06-04-31-14-301/output/sentiment-train', 'LocalPath': '/opt/ml/processing/output/sentiment/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'sentiment-validation', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-387108533973/sagemaker-scikit-learn-2021-07-06-04-31-14-301/output/sentiment-validation', 'LocalPath': '/opt/ml/processing/output/sentiment/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'sentiment-test', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-387108533973/sagemaker-scikit-learn-2021-07-06-04-31-14-301/output/sentiment-test', 'LocalPath': '/opt/ml/processing/output/sentiment/test', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "if (updated_correctly):\n",
    "\n",
    "    processor.run(code='src/prepare_data.py',\n",
    "              inputs=[\n",
    "                    ProcessingInput(source=raw_input_data_s3_uri,\n",
    "                                    destination='/opt/ml/processing/input/data/',\n",
    "                                    s3_data_distribution_type='ShardedByS3Key')\n",
    "              ],\n",
    "              outputs=[\n",
    "                    ProcessingOutput(output_name='sentiment-train',\n",
    "                                     source='/opt/ml/processing/output/sentiment/train',\n",
    "                                     s3_upload_mode='EndOfJob'),\n",
    "                    ProcessingOutput(output_name='sentiment-validation',\n",
    "                                     source='/opt/ml/processing/output/sentiment/validation',\n",
    "                                     s3_upload_mode='EndOfJob'),\n",
    "                    ProcessingOutput(output_name='sentiment-test',\n",
    "                                     source='/opt/ml/processing/output/sentiment/test',\n",
    "                                     s3_upload_mode='EndOfJob')\n",
    "              ],\n",
    "              arguments=['--train-split-percentage', str(train_split_percentage),\n",
    "                         '--validation-split-percentage', str(validation_split_percentage),\n",
    "                         '--test-split-percentage', str(test_split_percentage),\n",
    "                         '--balance-dataset', str(balance_dataset),\n",
    "                         '--max-seq-length', str(max_seq_length),                         \n",
    "                         '--feature-store-offline-prefix', str(feature_store_offline_prefix),\n",
    "                         '--feature-group-name', str(feature_group_name)                         \n",
    "              ],\n",
    "              logs=True,\n",
    "              wait=False)\n",
    "\n",
    "else:\n",
    "    print('#######################################')\n",
    "    print('Please update the code correctly above.')\n",
    "    print('#######################################')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the information about the processing jobs using the `describe` function. The result is in dictionary format. Let's pull the processing job name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing job name: sagemaker-scikit-learn-2021-07-06-04-31-14-301\n"
     ]
    }
   ],
   "source": [
    "scikit_processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n",
    "print('Processing job name: {}'.format(scikit_processing_job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-ex-3'></a>\n",
    "### Exercise 3\n",
    "\n",
    "Pull the processing job status from the processing job description.\n",
    "\n",
    "**Instructions**: Print the keys of the processing job description dictionary, choose the one related to the status of the processing job and print the value of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ProcessingInputs', 'ProcessingOutputConfig', 'ProcessingJobName', 'ProcessingResources', 'StoppingCondition', 'AppSpecification', 'Environment', 'RoleArn', 'ProcessingJobArn', 'ProcessingJobStatus', 'ProcessingStartTime', 'LastModifiedTime', 'CreationTime', 'ResponseMetadata'])\n",
      "Processing job status: sagemaker-scikit-learn-2021-07-06-04-31-14-301\n"
     ]
    }
   ],
   "source": [
    "print(processor.jobs[-1].describe().keys())\n",
    "\n",
    "### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "scikit_processing_job_status = processor.jobs[-1].describe()['ProcessingJobName'] # Replace None\n",
    "### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "print('Processing job status: {}'.format(scikit_processing_job_status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the created processing job in the AWS console.\n",
    "\n",
    "**Instructions**: \n",
    "- open the link\n",
    "- notice that you are in the section `Amazon SageMaker` -> `Processing jobs`\n",
    "- check the name of the processing job, its status and other available information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/processing-jobs/sagemaker-scikit-learn-2021-07-06-04-31-14-301\">processing job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/processing-jobs/{}\">processing job</a></b>'.format(region, scikit_processing_job_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for about 5 minutes to review the CloudWatch Logs. You may open the file [src/prepare_data.py](src/prepare_data.py) again and examine the outputs of the code in the CloudWatch logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=sagemaker-scikit-learn-2021-07-06-04-31-14-301;streamFilter=typeLogStreamPrefix\">CloudWatch logs</a> after about 5 minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch logs</a> after about 5 minutes</b>'.format(region, scikit_processing_job_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the completion of the processing job you can also review the output in the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-387108533973/sagemaker-scikit-learn-2021-07-06-04-31-14-301/?region=us-east-1&tab=overview\">S3 output data</a> after the processing job has completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 output data</a> after the processing job has completed</b>'.format(bucket, scikit_processing_job_name, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the processing job to complete.\n",
    "\n",
    "### _This cell will take approximately 15 minutes to run._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................!CPU times: user 439 ms, sys: 40.9 ms, total: 480 ms\n",
      "Wall time: 8min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(\n",
    "    processing_job_name=scikit_processing_job_name,\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "running_processor.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Please wait until ^^ Processing Job ^^ completes above_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the transformed and balanced data in the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-387108533973/sagemaker-scikit-learn-2021-07-06-04-31-14-301/output/sentiment-train\n",
      "s3://sagemaker-us-east-1-387108533973/sagemaker-scikit-learn-2021-07-06-04-31-14-301/output/sentiment-validation\n",
      "s3://sagemaker-us-east-1-387108533973/sagemaker-scikit-learn-2021-07-06-04-31-14-301/output/sentiment-test\n"
     ]
    }
   ],
   "source": [
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "output_config = processing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'sentiment-train':\n",
    "        processed_train_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'sentiment-validation':\n",
    "        processed_validation_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'sentiment-test':\n",
    "        processed_test_data_s3_uri = output['S3Output']['S3Uri']\n",
    "        \n",
    "print(processed_train_data_s3_uri)\n",
    "print(processed_validation_data_s3_uri)\n",
    "print(processed_test_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-06 04:44:14    4910638 part-algo-1-womens_clothing_ecommerce_reviews.tsv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_train_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-06 04:44:14     276404 part-algo-1-womens_clothing_ecommerce_reviews.tsv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_validation_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-06 04:44:15     272438 part-algo-1-womens_clothing_ecommerce_reviews.tsv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_test_data_s3_uri/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the data into the folder `balanced`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-387108533973/sagemaker-scikit-learn-2021-07-06-04-31-14-301/output/sentiment-train/part-algo-1-womens_clothing_ecommerce_reviews.tsv to balanced/sentiment-train/part-algo-1-womens_clothing_ecommerce_reviews.tsv\n",
      "download: s3://sagemaker-us-east-1-387108533973/sagemaker-scikit-learn-2021-07-06-04-31-14-301/output/sentiment-validation/part-algo-1-womens_clothing_ecommerce_reviews.tsv to balanced/sentiment-validation/part-algo-1-womens_clothing_ecommerce_reviews.tsv\n",
      "download: s3://sagemaker-us-east-1-387108533973/sagemaker-scikit-learn-2021-07-06-04-31-14-301/output/sentiment-test/part-algo-1-womens_clothing_ecommerce_reviews.tsv to balanced/sentiment-test/part-algo-1-womens_clothing_ecommerce_reviews.tsv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp $processed_train_data_s3_uri/part-algo-1-womens_clothing_ecommerce_reviews.tsv ./balanced/sentiment-train/\n",
    "!aws s3 cp $processed_validation_data_s3_uri/part-algo-1-womens_clothing_ecommerce_reviews.tsv ./balanced/sentiment-validation/\n",
    "!aws s3 cp $processed_test_data_s3_uri/part-algo-1-womens_clothing_ecommerce_reviews.tsv ./balanced/sentiment-test/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the training, validation and test data outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id\tsentiment\tlabel_id\tinput_ids\treview_body\tdate\n",
      "2518\t1\t2\t[0, 574, 12677, 42, 3588, 53, 2075, 203, 2514, 87, 97, 475, 4791, 548, 14442, 939, 33, 2740, 11, 5, 375, 328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tLoved this dress but ran much larger than other maeve dresses i have ordered in the past!\t2021-07-06T04:38:57Z\n",
      "4538\t0\t1\t[0, 100, 2638, 141, 42, 8443, 1415, 804, 8, 1276, 7, 645, 24, 4, 959, 1437, 39328, 64, 2128, 28, 12792, 13, 162, 98, 939, 2740, 258, 10, 4716, 1459, 3023, 29, 8, 4716, 1459, 37863, 29, 36, 118, 437, 195, 108, 134, 113, 1437, 14881, 19821, 1437, 8, 10, 11689, 1836, 2107, 740, 322, 939, 7428, 15, 5, 37863, 29, 78, 8, 21, 7358, 11, 24, 328, 98, 8366, 939, 802, 5, 3023, 29, 74, 28, 190, 2671, 131, 53, 7, 127, 2755, 5, 3023, 29, 2564, 169, 55, 41783, 328, 11, 5, 253, 1437, 939, 202, 399, 75, 101, 141, 7082, 5, 3023, 29, 2564, 98, 939, 1835, 106, 258, 4, 5, 1468, 16, 372, 1318, 8, 24, 18, 10, 11962, 1521, 4, 939, 74, 2]\t\"I loved how this jacket looked online and decided to order it. however  sizing can sometimes be tricky for me so i ordered both a petite xs and petite xxs (i'm 5'1\"\"  113 lb  and a bra size 32 c). i tired on the xxs first and was swimming in it! so naturally i thought the xs would be even bigger; but to my surprise the xs fit way more snug! in the end  i still didn't like how loose the xs fit so i returned them both. the material is great quality and it's a cute design. i would recommend trying t\"\t2021-07-06T04:38:57Z\n",
      "6982\t0\t1\t[0, 100, 770, 7, 657, 42, 3588, 98, 203, 53, 24, 95, 1415, 6664, 35187, 117, 948, 99, 939, 222, 4, 939, 437, 6329, 10, 501, 1437, 8, 939, 2162, 10, 1836, 316, 8, 24, 202, 1326, 182, 3298, 4740, 8, 45, 34203, 4, 939, 437, 5074, 142, 11, 2356, 24, 18, 127, 2674, 3588, 9, 70, 86, 328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tI wanted to love this dress so much but it just looked frumpy no matter what i did. i'm normally a 14  and i bought a size 12 and it still looks very baggy and not flattering. i'm sad because in photos it's my favorite dress of all time!\t2021-07-06T04:38:57Z\n",
      "2478\t-1\t0\t[0, 100, 2638, 42, 6399, 454, 5, 78, 86, 939, 15158, 24, 4, 24, 28704, 98, 203, 24, 1059, 10963, 48819, 4, 77, 939, 1835, 24, 5, 647, 5970, 26, 79, 56, 67, 2162, 42, 6399, 8, 5, 276, 631, 1102, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tI loved this shirt until the first time i washed it. it shrunk so much it became unwearable. when i returned it the salesperson said she had also bought this shirt and the same thing happened.\t2021-07-06T04:38:57Z\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 ./balanced/sentiment-train/part-algo-1-womens_clothing_ecommerce_reviews.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id\tsentiment\tlabel_id\tinput_ids\treview_body\tdate\n",
      "9277\t-1\t0\t[0, 133, 760, 6353, 2798, 16, 2198, 8818, 4, 118, 399, 75, 2229, 142, 127, 11689, 969, 8, 24, 74, 17948, 5, 356, 19, 10, 740, 5602, 12213, 4, 1437, 939, 2813, 5, 10199, 58, 42026, 111, 45, 98, 192, 149, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tThe front cream panel is completely transparent.i didn't purchase because my bra showed and it would ruin the look with a cami underneath.  i wish the fabric were translucent - not so see through.\t2021-07-06T04:38:57Z\n",
      "13975\t1\t2\t[0, 14721, 38093, 8, 11962, 4, 16106, 8089, 14, 64, 28, 10610, 19, 8077, 932, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tComfortable and cute. versatile colors that can be worn with virtually anything.\t2021-07-06T04:38:57Z\n",
      "5822\t0\t1\t[0, 100, 2740, 42, 11, 1104, 4, 5, 1468, 16, 1122, 7, 10, 8385, 139, 6399, 1437, 61, 939, 5844, 75, 11590, 15, 4, 24, 21, 67, 15, 5, 739, 1836, 8, 95, 29747, 24203, 15, 162, 4, 202, 1437, 5, 1318, 2551, 7, 28, 205, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tI ordered this in white. the material is similar to a polo shirt  which i hadn't counted on. it was also on the large size and just unflattering on me. still  the quality seemed to be good.\t2021-07-06T04:38:57Z\n",
      "2829\t1\t2\t[0, 133, 6306, 17187, 359, 1468, 36, 3654, 24, 17414, 23, 70, 7, 162, 43, 32, 30041, 13, 42, 2216, 23204, 73, 29582, 1437, 53, 5, 3989, 16, 45, 13, 162, 4, 939, 33, 10, 28701, 1119, 1437, 195, 108, 398, 113, 8325, 23246, 1437, 2631, 741, 7050, 36, 118, 1021, 2050, 3215, 127, 4505, 5278, 1437, 61, 2564, 43, 1437, 8, 5, 11052, 1438, 3398, 9, 42, 2125, 1415, 10861, 15, 162, 4, 77, 6148, 196, 1437, 5, 299, 457, 21, 15481, 15898, 5579, 45, 41783, 131, 959, 1437, 5, 795, 457, 14930, 196, 66, 7, 35633, 5593, 32934, 2072, 5, 28097, 528, 7, 5, 16415, 1295, 11, 124, 4, 5, 631, 16, 1437, 939, 33, 117, 28097, 1437, 8, 519, 28097, 16, 10, 182, 205, 2]\t\"The craftmanship & material (not itchy at all to me) are exquisite for this unique sweater/coat  but the shape is not for me. i have a slender build  5'8\"\" 130 lbs  34 b chest (i oredered my usual sm  which fit)  and the silouette of this piece looked ridiculous on me. when buttoned  the top half was nicely fitted-- not snug; however  the lower half ballooned out to overemphasize the hips due to the pleating in back. the thing is  i have no hips  and having hips is a very good thing  though not i\"\t2021-07-06T04:38:57Z\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 ./balanced/sentiment-validation/part-algo-1-womens_clothing_ecommerce_reviews.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id\tsentiment\tlabel_id\tinput_ids\treview_body\tdate\n",
      "4362\t0\t1\t[0, 713, 16, 10, 11962, 299, 1437, 53, 45, 966, 5, 425, 11, 127, 2979, 4, 939, 657, 5, 8089, 1437, 6184, 8, 3793, 1468, 341, 4, 959, 1437, 5, 10199, 16, 1341, 7174, 8, 95, 45, 966, 2746, 68, 4432, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tThis is a cute top  but not worth the price in my opinion. i love the colors  pattern and soft material used. however  the fabric is quite thin and just not worth paying $58.\t2021-07-06T04:38:57Z\n",
      "389\t1\t2\t[0, 100, 56, 24, 15, 127, 2813, 889, 13, 10, 251, 86, 1437, 45, 686, 549, 939, 197, 2229, 1437, 939, 190, 40591, 77, 24, 439, 15, 1392, 1437, 53, 2143, 1437, 524, 939, 7785, 939, 222, 645, 24, 734, 24, 16, 5373, 3473, 1437, 34203, 1437, 8, 657, 14, 24, 1326, 909, 53, 16, 2440, 734, 939, 2740, 258, 3023, 29, 8, 3023, 29, 181, 8, 1276, 7, 213, 19, 5, 4716, 1459, 25, 24, 40, 356, 2422, 11962, 11, 5, 1136, 19, 10317, 4, 45, 350, 765, 36, 118, 524, 195, 2767, 112, 4, 245, 44447, 5, 4486, 9219, 233, 2273, 162, 23, 78, 1437, 53, 939, 101, 24, 122, 734, 1085, 1099, 19313, 224, 59, 24, 328, 2, 1, 1, 1, 1, 1, 1, 1]\tI had it on my wish list for a long time  not sure whether i should purchase  i even hesitated when it went on sale  but boy  am i glad i did order it... it is crazy comfortable  flattering  and love that it looks black but is blue... i ordered both xs and xs p and decided to go with the petite as it will look super cute in the fall with boots. not too short (i am 5 foot 1.5)... the fake leather part concerned me at first  but i like it now... nothing bad ot say about it!\t2021-07-06T04:38:57Z\n",
      "2330\t-1\t0\t[0, 43567, 372, 15, 5, 1421, 1437, 53, 11, 3031, 1571, 89, 16, 169, 350, 203, 10199, 131, 99, 197, 28, 3041, 219, 8, 1087, 1722, 219, 1326, 101, 10, 10178, 36, 368, 10, 21592, 299, 15, 10, 4716, 1459, 693, 322, 24, 399, 75, 948, 61, 1836, 1437, 51, 70, 2564, 162, 15518, 352, 4, 939, 657, 5, 356, 8, 939, 802, 9, 2396, 24, 8, 519, 24, 551, 11, 1437, 53, 89, 18, 350, 203, 10199, 223, 5, 7050, 7, 146, 24, 41, 1365, 847, 73, 32815, 1258, 15, 5, 526, 39336, 4, 2128, 939, 5170, 54, 606, 62, 19, 209, 8117, 1437, 8, 114, 51, 7756, 5, 15331, 124, 15, 5, 3092, 4, 114, 47, 342, 10, 2, 1, 1, 1, 1, 1, 1, 1]\tLooks great on the model  but in actuality there is way too much fabric; what should be flowy and billowy looks like a tent (or a maternity top on a petite woman). it didn't matter which size  they all fit me disastrously. i love the look and i thought of keeping it and having it taken in  but there's too much fabric under the chest to make it an easy cut/alteration on the side seams. sometimes i wonder who comes up with these patterns  and if they pin the shirts back on the models. if you put a\t2021-07-06T04:38:57Z\n",
      "10660\t1\t2\t[0, 100, 300, 209, 11, 5, 7974, 3195, 359, 1335, 6238, 596, 51, 214, 31658, 25, 22, 462, 1438, 2590, 2753, 4352, 845, 114, 47, 214, 164, 7, 3568, 106, 66, 9, 5, 790, 1437, 3568, 10, 27115, 636, 50, 10, 3588, 81, 106, 4, 939, 26319, 5, 17940, 11842, 233, 62, 81, 127, 32606, 359, 5328, 106, 101, 9759, 17753, 77, 939, 439, 66, 4, 939, 300, 10, 475, 73, 462, 359, 51, 2564, 59, 15, 2242, 19, 97, 2084, 6149, 1033, 1589, 326, 6183, 9, 14, 1836, 1186, 4, 1437, 77, 939, 300, 184, 1437, 939, 6387, 106, 159, 359, 342, 5, 17539, 6806, 81, 127, 8872, 4, 5, 1079, 9, 5, 17539, 6806, 2850, 2962, 3804, 198, 127, 35713, 4, 939, 5940, 106, 13, 2]\t\"I got these in the neutral color & quickly understood why they're categorized as \"\"loungewear\"\". if you're going to wear them out of the house  wear a tunic or a dress over them. i folded the scrunch part up over my calves & wore them like boot socks when i went out. i got a m/l & they fit about on par with other leggings / tights of that size range.  when i got home  i rolled them down & put the bottoms over my heels. the rest of the bottoms scrunched around my ankles. i recommend them for around\"\t2021-07-06T04:38:57Z\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 ./balanced/sentiment-test/part-algo-1-womens_clothing_ecommerce_reviews.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-3.'></a>\n",
    "# 3. Query the Feature Store\n",
    "In addition to transforming the data and saving in S3 bucket, the processing job populates the feature store with the transformed and balanced data.  Let's query this data using Amazon Athena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-3.1.'></a>\n",
    "### 3.1. Export training, validation, and test datasets from the Feature Store\n",
    "\n",
    "Here you will do the export only for the training dataset, as an example. \n",
    "\n",
    "Use `athena_query()` function to create an Athena query for the defined above Feature Group. Then you can pull the table name of the Amazon Glue Data Catalog table which is auto-generated by Feature Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue Catalog table name: reviews-feature-group-1625545514-1625546302\n",
      "Running query: \n",
      "    SELECT date,\n",
      "        review_id,\n",
      "        sentiment, \n",
      "        label_id,\n",
      "        input_ids,\n",
      "        review_body\n",
      "    FROM \"reviews-feature-group-1625545514-1625546302\" \n",
      "    WHERE split_type='train' \n",
      "    LIMIT 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_store_query = feature_group.athena_query()\n",
    "\n",
    "feature_store_table = feature_store_query.table_name\n",
    "\n",
    "query_string = \"\"\"\n",
    "    SELECT date,\n",
    "        review_id,\n",
    "        sentiment, \n",
    "        label_id,\n",
    "        input_ids,\n",
    "        review_body\n",
    "    FROM \"{}\" \n",
    "    WHERE split_type='train' \n",
    "    LIMIT 5\n",
    "\"\"\".format(feature_store_table)\n",
    "\n",
    "print('Glue Catalog table name: {}'.format(feature_store_table))\n",
    "print('Running query: {}'.format(query_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the S3 location for the query results.  This allows us to re-use the query results for future queries if the data has not changed.  We can even share this S3 location between team members to improve query performance for common queries on data that does not change often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-387108533973/query_results/reviews-feature-store-1625545514/\n"
     ]
    }
   ],
   "source": [
    "output_s3_uri = 's3://{}/query_results/{}/'.format(bucket, feature_store_offline_prefix)\n",
    "print(output_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-ex-4'></a>\n",
    "### Exercise 4\n",
    "\n",
    "Query the feature store.\n",
    "\n",
    "**Instructions**: Use `feature_store_query.run` function passing the constructed above query string and the location of the output S3 bucket.\n",
    "\n",
    "```python\n",
    "feature_store_query.run(\n",
    "    query_string=..., # query string\n",
    "    output_location=... # location of the output S3 bucket\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_query.run(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    query_string=query_string, # Replace None\n",
    "    output_location=output_s3_uri # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    ")\n",
    "\n",
    "feature_store_query.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label_id</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-06T04:38:57Z</td>\n",
       "      <td>11013</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 100, 657, 5, 356, 9, 42, 3588, 359, 56, 239, 1991, 1437, 53, 24, 399, 75, 697, 62, 7, 106, 4...</td>\n",
       "      <td>I love the look of this dress &amp; had high hopes  but it didn't live up to them. have no doubt tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-07-06T04:38:57Z</td>\n",
       "      <td>5708</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 100, 1381, 15, 5, 6907, 11, 3023, 29, 11, 1400, 4, 24, 21, 98, 380, 15, 162, 4, 939, 770, 7,...</td>\n",
       "      <td>I tried on the pink in xs in store. it was so big on me. i wanted to love it  the stylist did  b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-07-06T04:38:57Z</td>\n",
       "      <td>15918</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 16587, 209, 13344, 328, 45, 350, 3229, 8, 45, 350, 765, 328, 51, 914, 98, 171, 13657, 328, 3...</td>\n",
       "      <td>Love these shorts! not too tight and not too short! they match so many tops! tts for my hard to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-07-06T04:38:57Z</td>\n",
       "      <td>6142</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 713, 23204, 95, 2035, 11, 127, 36661, 8, 24, 18, 416, 127, 2674, 2125, 9, 5296, 4, 24, 16, 5...</td>\n",
       "      <td>This sweater just arrived in my mailbox and it's already my favorite piece of clothing. it is th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-07-06T04:38:57Z</td>\n",
       "      <td>1410</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 713, 16, 10, 11962, 173, 8443, 1437, 25, 157, 25, 11153, 19, 10844, 4, 1437, 34203, 4, 182, ...</td>\n",
       "      <td>This is a cute work jacket  as well as paired with jeans.  flattering. very soft.  only con is t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date  review_id  sentiment  label_id  \\\n",
       "0  2021-07-06T04:38:57Z      11013          0         1   \n",
       "1  2021-07-06T04:38:57Z       5708          0         1   \n",
       "2  2021-07-06T04:38:57Z      15918          1         2   \n",
       "3  2021-07-06T04:38:57Z       6142          1         2   \n",
       "4  2021-07-06T04:38:57Z       1410          1         2   \n",
       "\n",
       "                                                                                             input_ids  \\\n",
       "0  [0, 100, 657, 5, 356, 9, 42, 3588, 359, 56, 239, 1991, 1437, 53, 24, 399, 75, 697, 62, 7, 106, 4...   \n",
       "1  [0, 100, 1381, 15, 5, 6907, 11, 3023, 29, 11, 1400, 4, 24, 21, 98, 380, 15, 162, 4, 939, 770, 7,...   \n",
       "2  [0, 16587, 209, 13344, 328, 45, 350, 3229, 8, 45, 350, 765, 328, 51, 914, 98, 171, 13657, 328, 3...   \n",
       "3  [0, 713, 23204, 95, 2035, 11, 127, 36661, 8, 24, 18, 416, 127, 2674, 2125, 9, 5296, 4, 24, 16, 5...   \n",
       "4  [0, 713, 16, 10, 11962, 173, 8443, 1437, 25, 157, 25, 11153, 19, 10844, 4, 1437, 34203, 4, 182, ...   \n",
       "\n",
       "                                                                                           review_body  \n",
       "0  I love the look of this dress & had high hopes  but it didn't live up to them. have no doubt tha...  \n",
       "1  I tried on the pink in xs in store. it was so big on me. i wanted to love it  the stylist did  b...  \n",
       "2  Love these shorts! not too tight and not too short! they match so many tops! tts for my hard to ...  \n",
       "3  This sweater just arrived in my mailbox and it's already my favorite piece of clothing. it is th...  \n",
       "4  This is a cute work jacket  as well as paired with jeans.  flattering. very soft.  only con is t...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"max_colwidth\", 100)\n",
    "\n",
    "df_feature_store = feature_store_query.as_dataframe()\n",
    "df_feature_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the Feature Store in SageMaker Studio\n",
    "\n",
    "![](images/sm_studio_extensions_featurestore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-3.2.'></a>\n",
    "### 3.2. Export TSV from Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the output as a TSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_store.to_csv('./feature_store_export.tsv',\n",
    "                        sep='\\t',\n",
    "                        index=False,\n",
    "                        header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\treview_id\tsentiment\tlabel_id\tinput_ids\treview_body\n",
      "2021-07-06T04:38:57Z\t11013\t0\t1\t[0, 100, 657, 5, 356, 9, 42, 3588, 359, 56, 239, 1991, 1437, 53, 24, 399, 75, 697, 62, 7, 106, 4, 33, 117, 2980, 14, 24, 16, 157, 156, 359, 2721, 1437, 1950, 9321, 19, 9869, 14901, 198, 5, 5397, 4, 939, 524, 195, 108, 401, 113, 1437, 18918, 29882, 19, 10, 618, 7755, 783, 18951, 14, 939, 1017, 101, 7, 7433, 1437, 98, 939, 4689, 42, 3588, 25, 5, 6173, 224, 24, 18, 10, 828, 7082, 359, 36341, 4, 939, 303, 24, 7, 28, 98, 7082, 14, 24, 1326, 380, 359, 4378, 35162, 127, 1955, 328, 939, 1687, 12926, 10, 650, 1437, 53, 1467, 14, 24, 21, 1622, 5, 847, 9, 5, 3588, 359, 164, 159, 10, 1836, 1979, 75, 24622, 2, 1, 1, 1, 1]\t\"I love the look of this dress & had high hopes  but it didn't live up to them. have no doubt that it is well made & beautiful  fully lined with lovely detailing around the neck. i am 5'6\"\"  155lbs with a postpartum belly that i'd like to hide  so i chose this dress as the reviews say it's a bit loose & forgiving. i found it to be so loose that it looks big & entirely hides my figure! i considered ordering a small  but knew that it was simply the cut of the dress & going down a size wouldn't chang\"\n",
      "2021-07-06T04:38:57Z\t5708\t0\t1\t[0, 100, 1381, 15, 5, 6907, 11, 3023, 29, 11, 1400, 4, 24, 21, 98, 380, 15, 162, 4, 939, 770, 7, 657, 24, 1437, 5, 15240, 661, 222, 1437, 53, 939, 1299, 162, 298, 734, 98, 222, 277, 701, 26229, 1437, 399, 947, 10431, 3416, 131, 90, 101, 24, 15, 162, 4, 5, 8089, 32, 269, 2579, 36, 463, 5, 2440, 1302, 269, 2579, 350, 43, 1437, 53, 5, 847, 21, 10, 410, 1810, 4, 10199, 16, 182, 3793, 4, 13, 5135, 1437, 939, 524, 12312, 23246, 1437, 321, 16134, 1437, 973, 4, 245, 11, 13977, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tI tried on the pink in xs in store. it was so big on me. i wanted to love it  the stylist did  but i felt meh... so did another costumer  didn&#39;t like it on me. the colors are really nice (and the blue seems really nice too)  but the cut was a little wide. fabric is very soft. for reference  i am 115 lbs  0dd  26.5 in waist.\n",
      "2021-07-06T04:38:57Z\t15918\t1\t2\t[0, 16587, 209, 13344, 328, 45, 350, 3229, 8, 45, 350, 765, 328, 51, 914, 98, 171, 13657, 328, 326, 1872, 13, 127, 543, 7, 2564, 1955, 328, 372, 13, 12729, 1437, 28177, 1437, 8, 95, 7, 28, 15652, 11, 328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tLove these shorts! not too tight and not too short! they match so many tops! tts for my hard to fit figure! great for weekends  vacations  and just to be adorable in!\n",
      "2021-07-06T04:38:57Z\t6142\t1\t2\t[0, 713, 23204, 95, 2035, 11, 127, 36661, 8, 24, 18, 416, 127, 2674, 2125, 9, 5296, 4, 24, 16, 5, 3793, 990, 631, 939, 308, 734, 8, 24, 2653, 98, 30847, 242, 4, 25, 5, 37102, 874, 1581, 35, 4420, 1437, 24, 34, 12189, 4, 51, 32, 34915, 12, 5827, 12189, 36, 27737, 596, 47, 64, 75, 192, 106, 11, 5, 1345, 43, 8, 5, 1273, 16, 20856, 19, 5, 276, 6129, 4617, 3520, 15, 5, 760, 9, 5, 23204, 4, 1437, 939, 437, 67, 10, 3097, 734, 53, 939, 202, 120, 22037, 13, 10, 12003, 1294, 98, 939, 218, 75, 1508, 8165, 22, 22376, 3361, 17707, 72, 939, 581, 1153, 3568, 42, 7, 3553, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\t\"This sweater just arrived in my mailbox and it's already my favorite piece of clothing. it is the softest thing i own... and it feels so luxe. as the reviewer below noted: yes  it has pockets. they are slit-style pockets (probably why you can't see them in the photo) and the opening is trimmed with the same cable detail featured on the front of the sweater.  i'm also a professor... but i still get mistaken for a grad student so i don't mind appearing \"\"professorial.\"\" i'll probably wear this to th\"\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 ./feature_store_export.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload TSV to the S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./feature_store_export.tsv to s3://sagemaker-us-east-1-387108533973/feature_store/feature_store_export.tsv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./feature_store_export.tsv s3://$bucket/feature_store/feature_store_export.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the file in the S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-06 04:56:36       4789 feature_store/feature_store_export.tsv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --recursive s3://$bucket/feature_store/feature_store_export.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-3.3.'></a>\n",
    "### 3.3. Check that the dataset in the Feature Store is balanced by sentiment\n",
    "\n",
    "Now you can setup an Athena query to check that the stored dataset is balanced by the target class `sentiment`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-ex-5'></a>\n",
    "### Exercise 5\n",
    "\n",
    "Write an SQL query to count the total number of the reviews per `sentiment` stored in the Feature Group.\n",
    "\n",
    "**Instructions**: Pass the SQL statement of the form \n",
    "\n",
    "```sql\n",
    "SELECT category_column, COUNT(*) AS new_column_name\n",
    "FROM table_name\n",
    "GROUP BY category_column\n",
    "```\n",
    "\n",
    "into the variable `query_string_count_by_sentiment`. Here you would need to use the column `sentiment` and give a name `count_reviews` to the new column with the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9602327c6186>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_store_query_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mathena_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Replace all None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m query_string_count_by_sentiment = \"\"\"\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query_string' is not defined"
     ]
    }
   ],
   "source": [
    "feature_store_query_2 = query_string.athena_query()\n",
    "\n",
    "# Replace all None\n",
    "### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "query_string_count_by_sentiment = \"\"\"\n",
    "SELECT sentiment, COUNT(*) AS count_reviews\n",
    "FROM reviews-feature-group-1625545514-1625546302\n",
    "GROUP BY sentiment\n",
    "\"\"\".format(feature_store_table)\n",
    "### END SOLUTION - DO NOT delete this comment for grading purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-ex-6'></a>\n",
    "### Exercise 6\n",
    "\n",
    "Query the feature store.\n",
    "\n",
    "**Instructions**: Use `run` function of the Feature Store query, passing the new query string `query_string_count_by_sentiment`. The output S3 bucket will remain unchanged. You can follow the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_store_query_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-797a2a8d3702>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m feature_store_query_2.run(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mquery_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_string_count_by_sentiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Replace None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutput_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;31m# Replace None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m### END SOLUTION - DO NOT delete this comment for grading purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_store_query_2' is not defined"
     ]
    }
   ],
   "source": [
    "feature_store_query_2.run(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    query_string=query_string_count_by_sentiment, # Replace None\n",
    "    output_location=None # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    ")\n",
    "\n",
    "feature_store_query_2.wait()\n",
    "\n",
    "df_count_by_sentiment = feature_store_query_2.as_dataframe()\n",
    "df_count_by_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-ex-7'></a>\n",
    "### Exercise 7\n",
    "\n",
    "Visualize the result of the query in the bar plot, showing the count of the reviews by sentiment value.\n",
    "\n",
    "**Instructions**: Pass the resulting data frame `df_count_by_sentiment` into the `barplot` function of the `seaborn` library.\n",
    "\n",
    "```python\n",
    "sns.barplot(\n",
    "    data=..., \n",
    "    x='...', \n",
    "    y='...',\n",
    "    color=\"blue\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    data=None, # Replace None\n",
    "    x=None, # Replace None\n",
    "    y=None, # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    color=\"blue\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the notebook and `prepare_data.py` file into S3 bucket for grading purposes.\n",
    "\n",
    "**Note**: you may need to save the file before the upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./C2_W1_Assignment.ipynb s3://$bucket/C2_W1_Assignment_Learner.ipynb\n",
    "!aws s3 cp ./src/prepare_data.py s3://$bucket/src/C2_W1_prepare_data_Learner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please go to the main lab window and click on `Submit` button (see the `Finish the lab` section of the instructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
